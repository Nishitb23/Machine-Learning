# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RBlRe0eQZgkD0CkDE-Bf-yC4OwjrK_EW
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import math
import random

df = pd.read_csv("bill_authentication.csv")

#extracting the features from dataset and appending column to make it homogeneous
features = df.iloc[:,:-1].replace(math.nan,0)
features['bias'] = 1;

#extracting labels 
labels = df.iloc[:,-1:].replace(0,-1)

#converting features and labels to matrix form
fvector = np.array(features)
lvector = np.array(labels)

#calculating number of features per example
featuresSize = features.shape[1]

#data split into test and training data
splitPercent = [0.3,0.2,0.1];

for percent in splitPercent:
  xtrain, xtest, y_train, y_test = train_test_split(fvector, lvector, test_size= percent,shuffle = True)

  #feature scaling for test and training inputs
  
  sc_x = StandardScaler() 
  X_train = sc_x.fit_transform(xtrain)  
  X_test = sc_x.transform(xtest)

  #initializing free parameter to all zero's
  w = np.zeros([featuresSize,1])
  trainingSize = X_train.shape[0]

  #setting the optimum number of iterations required

  for t in range(0,400):
    count=0
    for i in range(0,trainingSize):
     if float(y_train[i]*np.dot(X_train[i],w)) <= 0.0 :
       count+=1
       w = w+(y_train[i]* (X_train[i]).reshape(-1,1))
    
    if count == 0 :
      break;
  
  #testing efficiency on test data
  totalCases=X_test.shape[0]
  misclassify = 0
  for i in range(0,totalCases):
    if float(y_test[i]*np.dot(X_test[i],w)) <= 0.0:
      misclassify+=1
  print("Free Parameters: ",w.T)
  print("Efficiency for",int((1-percent)*100),":",
        int(percent*100),"split is: ",round((1-misclassify/totalCases)*100,2),"%")
  
print("Since the data is linearly non-seperable efficiency is low")
